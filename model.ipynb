{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import tflearn\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected, flatten\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d, avg_pool_2d\n",
    "from tflearn.layers.estimator import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train.zip\n",
      "Extracted train.zip\n",
      "Extracting test.zip\n",
      "Extracted test.zip\n"
     ]
    }
   ],
   "source": [
    "# Parent folder where train and test images are saved\n",
    "main_dir = '/home/ubuntu/pynb/kaggle-ndsb1/data'\n",
    "\n",
    "def extract_data(filename, output_dir):\n",
    "    '''\n",
    "    Extracts zipped train and test files\n",
    "    '''\n",
    "    # Change working directory to the one containing train.zip & test.zip\n",
    "    os.chdir(output_dir)\n",
    "    \n",
    "    # Extracts zip files\n",
    "    print (\"Extracting \" + filename)\n",
    "    \n",
    "    fh = open(filename, 'rb')\n",
    "    z = zipfile.ZipFile(fh)\n",
    "    for name in z.namelist():\n",
    "        z.extract(name, output_dir)\n",
    "    fh.close()\n",
    "    print (\"Extracted \" + filename)\n",
    "        \n",
    "extract_data(\"train.zip\", main_dir)\n",
    "extract_data(\"test.zip\", main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing . . .\n",
      "Done data augmentation.\n"
     ]
    }
   ],
   "source": [
    "# Build numpy array of images, resize to 48x48 and shuffle dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "\n",
    "main_dir = '/home/ubuntu/pynb/kaggle-ndsb1/data'\n",
    "# Output folder where augmented images will be saved\n",
    "aug_dir = '/home/ubuntu/pynb/kaggle-ndsb1/data/aug'\n",
    "\n",
    "def img_aug(main_dir, aug_dir):\n",
    "    '''\n",
    "    Read images with OpenCV, perform augmentation and save the augmented images on disk\n",
    "    '''\n",
    "    # Create aug dir if does not exist\n",
    "    if not os.path.exists(aug_dir):\n",
    "        os.makedirs(aug_dir)\n",
    "    print (\"Initializing . . .\")\n",
    "    # Iterate over image classes\n",
    "    for pclass in os.listdir(main_dir + \"/train\"):\n",
    "        # Create aug/class dir if does not exist\n",
    "        if not os.path.exists(aug_dir + \"/\" + pclass):\n",
    "            os.makedirs(aug_dir + \"/\" + pclass)\n",
    "        # Iterate over images in each class\n",
    "        for pimg in os.listdir(main_dir + \"/train/\" + pclass):\n",
    "            img = cv2.imread(main_dir + '/train/' + pclass + '/' + pimg)\n",
    "            res = cv2.resize(img, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "            cv_gray = cv2.cvtColor(res, cv2.COLOR_RGB2GRAY)\n",
    "            rows,cols = cv_gray.shape\n",
    "            # Save base resized and grayscaled image\n",
    "            cv2.imwrite(aug_dir + \"/\" + pclass + \"/\" + pimg[:-4] + \"_base.jpg\", cv_gray)\n",
    "            # Perform 3 rotations and save them to disk separately\n",
    "            M1 = cv2.getRotationMatrix2D((cols/2,rows/2),90,1)\n",
    "            dst1 = cv2.warpAffine(cv_gray,M1,(cols,rows))\n",
    "            cv2.imwrite(aug_dir + \"/\" + pclass + \"/\" + pimg[:-4] + \"_aug1.jpg\", dst1)\n",
    "            M2 = cv2.getRotationMatrix2D((cols/2,rows/2),180,1)\n",
    "            dst2 = cv2.warpAffine(cv_gray,M2,(cols,rows))\n",
    "            cv2.imwrite(aug_dir + \"/\" + pclass + \"/\" + pimg[:-4] + \"_aug2.jpg\", dst2)\n",
    "            M3 = cv2.getRotationMatrix2D((cols/2,rows/2),270,1)\n",
    "            dst3 = cv2.warpAffine(cv_gray,M3,(cols,rows))\n",
    "            cv2.imwrite(aug_dir + \"/\" + pclass + \"/\" + pimg[:-4] + \"_aug3.jpg\", dst3)\n",
    "    print (\"Done data augmentation.\")\n",
    "\n",
    "img_aug(main_dir, aug_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing . . .\n",
      "Images sucessfully converted to dict.\n",
      "121344\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "aug_dir = '/home/ubuntu/pynb/kaggle-ndsb1/data/aug'\n",
    "\n",
    "def img_to_dict(aug_dir):\n",
    "    '''\n",
    "    Read images with OpenCV, perform transformation and save in dict\n",
    "    '''\n",
    "    print (\"Initializing . . .\")\n",
    "    # Initialize empty dict to contain each image as an array\n",
    "    img_dict = {}\n",
    "    # Iterate over image classes\n",
    "    for pclass in os.listdir(aug_dir):\n",
    "        # Iterate over images in a class\n",
    "        for pimg in os.listdir(aug_dir + \"/\" + pclass):\n",
    "            img = cv2.imread(aug_dir + '/' + pclass + '/' + pimg)\n",
    "            cv_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "            img_dict[pimg] = {pclass: cv_gray}\n",
    "    print (\"Images sucessfully converted to dict.\")\n",
    "    return img_dict\n",
    "\n",
    "img_dict = img_to_dict(aug_dir)\n",
    "print len(img_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acantharia_protist': array([[255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       ..., \n",
      "       [254, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 254, 254, ..., 255, 255, 255]], dtype=uint8)}\n"
     ]
    }
   ],
   "source": [
    "print img_dict['907_base.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "# Save dict\n",
    "import pickle\n",
    "with open('img_dict_2.pickle', 'wb') as handle:\n",
    "    pickle.dump(img_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print (\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load dict\n",
    "import pickle\n",
    "with open('img_dict_2.pickle', 'rb') as handle:\n",
    "    img_dict = pickle.load(handle)\n",
    "print (\"File loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((121344, 48, 48, 1), (121344, 121))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_dataset(img_dict):\n",
    "    '''\n",
    "    Constructs dataset of images as array and its labels.\n",
    "    Also normalizes pixel values by mean and std deviation.\n",
    "    '''\n",
    "    labels = [] # Need to change labels to numpy array\n",
    "    dataset = np.ndarray([len(img_dict),48,48,1], dtype='float32')\n",
    "    i = 0\n",
    "    # Iterate image and its class-array values pair\n",
    "    for img, pclass_arr in img_dict.iteritems():\n",
    "        \n",
    "        labels.append(pclass_arr.keys()[0])\n",
    "        im_array = pclass_arr.values()[0].reshape((48,48,1))\n",
    "        mean = np.mean(im_array, dtype='float32')\n",
    "        std = np.std(im_array, dtype='float32')\n",
    "        # Apply normalization to each image\n",
    "        im_array = (im_array - mean) / std \n",
    "        dataset[i,:,:,:] = im_array\n",
    "        i += 1\n",
    "        \n",
    "    # Encode labels\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    lb = LabelBinarizer()\n",
    "    labels = lb.fit_transform(labels)\n",
    "    \n",
    "    return dataset, labels\n",
    "    \n",
    "dataset, labels = prepare_dataset(img_dict)\n",
    "print (dataset.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-6.74140358]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  ..., \n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]]\n",
      "\n",
      " [[-6.74140358]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  ..., \n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]]\n",
      "\n",
      " [[-6.74140358]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  ..., \n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]]\n",
      "\n",
      " ..., \n",
      " [[-6.74140358]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  ..., \n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]]\n",
      "\n",
      " [[-6.74140358]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  ..., \n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]]\n",
      "\n",
      " [[-6.74140358]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  ..., \n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]]]\n"
     ]
    }
   ],
   "source": [
    "print dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "# Save dataset and labels\n",
    "import pickle\n",
    "with open('dataset.pickle', 'wb') as handle:\n",
    "    pickle.dump(dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(labels, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print (\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and labels\n",
    "import pickle\n",
    "with open('dataset.pickle', 'rb') as handle:\n",
    "    img_dict = pickle.load(handle)\n",
    "with open('labels.pickle', 'rb') as handle:\n",
    "    img_dict = pickle.load(handle)\n",
    "print (\"File loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([67139, 80980, 74225, ..., 12282, 35638, 11316]), 'VAL:', array([115058, 119818, 104462, ...,  85887,  69888,  18654]))\n",
      "('TRAIN:', array([ 65522,  39899, 116935, ..., 114485,  48610,  97844]), 'VAL:', array([81710, 73998, 38975, ..., 29230, 60841, 73892]))\n",
      "('TRAIN:', array([13844, 41144, 53608, ...,  4187, 87474, 81950]), 'VAL:', array([117790,  17974,  59888, ...,  18291,  39433,  94389]))\n"
     ]
    }
   ],
   "source": [
    "# Split training data to train and validation, then shuffle datasets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=3, test_size=0.1, random_state=42)\n",
    "\n",
    "for train_index, val_index in sss.split(dataset, labels):\n",
    "    print(\"TRAIN:\", train_index, \"VAL:\", val_index)\n",
    "    X_train, X_val = dataset[train_index], dataset[val_index]\n",
    "    y_train, y_val = labels[train_index], labels[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "# Save dataset and labels\n",
    "import pickle\n",
    "with open('train.pickle', 'wb') as handle:\n",
    "    pickle.dump([X_train, y_train], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('val.pickle', 'wb') as handle:\n",
    "    pickle.dump([X_val, y_val], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print (\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and labels\n",
    "import pickle\n",
    "with open('train.pickle', 'rb') as handle:\n",
    "    X_train, y_train = pickle.load(handle)\n",
    "with open('val.pickle', 'rb') as handle:\n",
    "    X_val, y_val = pickle.load(handle)\n",
    "print (\"File loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name image_preloader",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dc7337e95888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build the preloader array, resize images to 48x48, and shuffle dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage_preloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m X, Y = image_preloader('data/train/', image_shape=(48, 48), mode='folder',\n\u001b[1;32m      5\u001b[0m                        categorical_labels=True, grayscale=True, normalize=False)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name image_preloader"
     ]
    }
   ],
   "source": [
    "# Build the preloader array, resize images to 48x48, and shuffle dataset\n",
    "# Note: Error on importing preloader array since 1 Mar 2017. Thus, manual data aug was performed instead\n",
    "'''from tflearn.data_utils import image_preloader\n",
    "from tflearn.data_utils import shuffle\n",
    "X, Y = image_preloader('data/train/', image_shape=(48, 48), mode='folder',\n",
    "                       categorical_labels=True, grayscale=True, normalize=False)\n",
    "X, Y = shuffle(X, Y)\n",
    "\n",
    "print (X[0], Y[0])\n",
    "\n",
    "# Preprocess images\n",
    "img_prep = tflearn.ImagePreprocessing()\n",
    "# Zero Center (With mean computed over the whole dataset)\n",
    "img_prep.add_featurewise_zero_center()\n",
    "# STD Normalization (With std computed over the whole dataset)\n",
    "img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "# Create extra synthetic training data by flipping, rotating and blurring the\n",
    "# images on our data set.\n",
    "img_aug = tflearn.ImageAugmentation()\n",
    "img_aug.add_random_flip_leftright()\n",
    "img_aug.add_random_flip_updown()\n",
    "img_aug.add_random_rotation(max_angle=25.)\n",
    "img_aug.add_random_blur(sigma_max=2.)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 34130  | total loss: \u001b[1m\u001b[32m1.43946\u001b[0m\u001b[0m\n",
      "\u001b[2K\r",
      "| Momentum | epoch: 010 | loss: 1.43946 - acc: 0.5640 -- iter: 109209/109209\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected, flatten\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d, avg_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.merge_ops import merge\n",
    "\n",
    "# Model\n",
    "'''network = input_data(shape=[None, 48, 48, 1])\n",
    "\n",
    "network = conv_2d(network, 32, 5, activation='relu', padding='valid')\n",
    "network = conv_2d(network, 64, 5, activation='relu', padding='valid')\n",
    "network = max_pool_2d(network, 3, strides=2)\n",
    "\n",
    "network = conv_2d(network, 64, 3, activation='relu')\n",
    "network = conv_2d(network, 64, 3, activation='relu')\n",
    "network = conv_2d(network, 128, 3, activation='relu')\n",
    "network = max_pool_2d(network, 3, strides=2)\n",
    "\n",
    "network = conv_2d(network, 256, 3, activation='relu')\n",
    "network = conv_2d(network, 256, 3, activation='relu')\n",
    "network = avg_pool_2d(network, 9, strides=1)\n",
    "\n",
    "network = flatten(network)\n",
    "network = dropout(network, 0.25)\n",
    "network = fully_connected(network, 121, activation='softmax')\n",
    "\n",
    "network = regression(network, optimizer='rmsprop',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 learning_rate=0.001)'''\n",
    "\n",
    "# Model - Inception V3\n",
    "network = input_data(shape=[None, 48, 48, 1])\n",
    "conv1_7_7 = conv_2d(network, 64, 7, strides=2, activation='relu', name = 'conv1_7_7_s2')\n",
    "pool1_3_3 = max_pool_2d(conv1_7_7, 3,strides=2)\n",
    "pool1_3_3 = local_response_normalization(pool1_3_3)\n",
    "conv2_3_3_reduce = conv_2d(pool1_3_3, 64,1, activation='relu',name = 'conv2_3_3_reduce')\n",
    "conv2_3_3 = conv_2d(conv2_3_3_reduce, 192,3, activation='relu', name='conv2_3_3')\n",
    "conv2_3_3 = local_response_normalization(conv2_3_3)\n",
    "pool2_3_3 = max_pool_2d(conv2_3_3, kernel_size=3, strides=2, name='pool2_3_3_s2')\n",
    "inception_3a_1_1 = conv_2d(pool2_3_3, 64, 1, activation='relu', name='inception_3a_1_1')\n",
    "inception_3a_3_3_reduce = conv_2d(pool2_3_3, 96,1, activation='relu', name='inception_3a_3_3_reduce')\n",
    "inception_3a_3_3 = conv_2d(inception_3a_3_3_reduce, 128,filter_size=3,  activation='relu', name = 'inception_3a_3_3')\n",
    "inception_3a_5_5_reduce = conv_2d(pool2_3_3,16, filter_size=1,activation='relu', name ='inception_3a_5_5_reduce' )\n",
    "inception_3a_5_5 = conv_2d(inception_3a_5_5_reduce, 32, filter_size=5, activation='relu', name= 'inception_3a_5_5')\n",
    "inception_3a_pool = max_pool_2d(pool2_3_3, kernel_size=3, strides=1, )\n",
    "inception_3a_pool_1_1 = conv_2d(inception_3a_pool, 32, filter_size=1, activation='relu', name='inception_3a_pool_1_1')\n",
    "\n",
    "# merge the inception_3a__\n",
    "inception_3a_output = merge([inception_3a_1_1, inception_3a_3_3, inception_3a_5_5, inception_3a_pool_1_1], mode='concat', axis=3)\n",
    "\n",
    "inception_3b_1_1 = conv_2d(inception_3a_output, 128,filter_size=1,activation='relu', name= 'inception_3b_1_1' )\n",
    "inception_3b_3_3_reduce = conv_2d(inception_3a_output, 128, filter_size=1, activation='relu', name='inception_3b_3_3_reduce')\n",
    "inception_3b_3_3 = conv_2d(inception_3b_3_3_reduce, 192, filter_size=3,  activation='relu',name='inception_3b_3_3')\n",
    "inception_3b_5_5_reduce = conv_2d(inception_3a_output, 32, filter_size=1, activation='relu', name = 'inception_3b_5_5_reduce')\n",
    "inception_3b_5_5 = conv_2d(inception_3b_5_5_reduce, 96, filter_size=5,  name = 'inception_3b_5_5')\n",
    "inception_3b_pool = max_pool_2d(inception_3a_output, kernel_size=3, strides=1,  name='inception_3b_pool')\n",
    "inception_3b_pool_1_1 = conv_2d(inception_3b_pool, 64, filter_size=1,activation='relu', name='inception_3b_pool_1_1')\n",
    "\n",
    "#merge the inception_3b_*\n",
    "inception_3b_output = merge([inception_3b_1_1, inception_3b_3_3, inception_3b_5_5, inception_3b_pool_1_1], mode='concat',axis=3,name='inception_3b_output')\n",
    "\n",
    "pool3_3_3 = max_pool_2d(inception_3b_output, kernel_size=3, strides=2, name='pool3_3_3')\n",
    "inception_4a_1_1 = conv_2d(pool3_3_3, 192, filter_size=1, activation='relu', name='inception_4a_1_1')\n",
    "inception_4a_3_3_reduce = conv_2d(pool3_3_3, 96, filter_size=1, activation='relu', name='inception_4a_3_3_reduce')\n",
    "inception_4a_3_3 = conv_2d(inception_4a_3_3_reduce, 208, filter_size=3,  activation='relu', name='inception_4a_3_3')\n",
    "inception_4a_5_5_reduce = conv_2d(pool3_3_3, 16, filter_size=1, activation='relu', name='inception_4a_5_5_reduce')\n",
    "inception_4a_5_5 = conv_2d(inception_4a_5_5_reduce, 48, filter_size=5,  activation='relu', name='inception_4a_5_5')\n",
    "inception_4a_pool = max_pool_2d(pool3_3_3, kernel_size=3, strides=1,  name='inception_4a_pool')\n",
    "inception_4a_pool_1_1 = conv_2d(inception_4a_pool, 64, filter_size=1, activation='relu', name='inception_4a_pool_1_1')\n",
    "\n",
    "inception_4a_output = merge([inception_4a_1_1, inception_4a_3_3, inception_4a_5_5, inception_4a_pool_1_1], mode='concat', axis=3, name='inception_4a_output')\n",
    "\n",
    "\n",
    "inception_4b_1_1 = conv_2d(inception_4a_output, 160, filter_size=1, activation='relu', name='inception_4a_1_1')\n",
    "inception_4b_3_3_reduce = conv_2d(inception_4a_output, 112, filter_size=1, activation='relu', name='inception_4b_3_3_reduce')\n",
    "inception_4b_3_3 = conv_2d(inception_4b_3_3_reduce, 224, filter_size=3, activation='relu', name='inception_4b_3_3')\n",
    "inception_4b_5_5_reduce = conv_2d(inception_4a_output, 24, filter_size=1, activation='relu', name='inception_4b_5_5_reduce')\n",
    "inception_4b_5_5 = conv_2d(inception_4b_5_5_reduce, 64, filter_size=5,  activation='relu', name='inception_4b_5_5')\n",
    "\n",
    "inception_4b_pool = max_pool_2d(inception_4a_output, kernel_size=3, strides=1,  name='inception_4b_pool')\n",
    "inception_4b_pool_1_1 = conv_2d(inception_4b_pool, 64, filter_size=1, activation='relu', name='inception_4b_pool_1_1')\n",
    "\n",
    "inception_4b_output = merge([inception_4b_1_1, inception_4b_3_3, inception_4b_5_5, inception_4b_pool_1_1], mode='concat', axis=3, name='inception_4b_output')\n",
    "\n",
    "\n",
    "inception_4c_1_1 = conv_2d(inception_4b_output, 128, filter_size=1, activation='relu',name='inception_4c_1_1')\n",
    "inception_4c_3_3_reduce = conv_2d(inception_4b_output, 128, filter_size=1, activation='relu', name='inception_4c_3_3_reduce')\n",
    "inception_4c_3_3 = conv_2d(inception_4c_3_3_reduce, 256,  filter_size=3, activation='relu', name='inception_4c_3_3')\n",
    "inception_4c_5_5_reduce = conv_2d(inception_4b_output, 24, filter_size=1, activation='relu', name='inception_4c_5_5_reduce')\n",
    "inception_4c_5_5 = conv_2d(inception_4c_5_5_reduce, 64,  filter_size=5, activation='relu', name='inception_4c_5_5')\n",
    "\n",
    "inception_4c_pool = max_pool_2d(inception_4b_output, kernel_size=3, strides=1)\n",
    "inception_4c_pool_1_1 = conv_2d(inception_4c_pool, 64, filter_size=1, activation='relu', name='inception_4c_pool_1_1')\n",
    "\n",
    "inception_4c_output = merge([inception_4c_1_1, inception_4c_3_3, inception_4c_5_5, inception_4c_pool_1_1], mode='concat', axis=3,name='inception_4c_output')\n",
    "\n",
    "inception_4d_1_1 = conv_2d(inception_4c_output, 112, filter_size=1, activation='relu', name='inception_4d_1_1')\n",
    "inception_4d_3_3_reduce = conv_2d(inception_4c_output, 144, filter_size=1, activation='relu', name='inception_4d_3_3_reduce')\n",
    "inception_4d_3_3 = conv_2d(inception_4d_3_3_reduce, 288, filter_size=3, activation='relu', name='inception_4d_3_3')\n",
    "inception_4d_5_5_reduce = conv_2d(inception_4c_output, 32, filter_size=1, activation='relu', name='inception_4d_5_5_reduce')\n",
    "inception_4d_5_5 = conv_2d(inception_4d_5_5_reduce, 64, filter_size=5,  activation='relu', name='inception_4d_5_5')\n",
    "inception_4d_pool = max_pool_2d(inception_4c_output, kernel_size=3, strides=1,  name='inception_4d_pool')\n",
    "inception_4d_pool_1_1 = conv_2d(inception_4d_pool, 64, filter_size=1, activation='relu', name='inception_4d_pool_1_1')\n",
    "\n",
    "inception_4d_output = merge([inception_4d_1_1, inception_4d_3_3, inception_4d_5_5, inception_4d_pool_1_1], mode='concat', axis=3, name='inception_4d_output')\n",
    "\n",
    "inception_4e_1_1 = conv_2d(inception_4d_output, 256, filter_size=1, activation='relu', name='inception_4e_1_1')\n",
    "inception_4e_3_3_reduce = conv_2d(inception_4d_output, 160, filter_size=1, activation='relu', name='inception_4e_3_3_reduce')\n",
    "inception_4e_3_3 = conv_2d(inception_4e_3_3_reduce, 320, filter_size=3, activation='relu', name='inception_4e_3_3')\n",
    "inception_4e_5_5_reduce = conv_2d(inception_4d_output, 32, filter_size=1, activation='relu', name='inception_4e_5_5_reduce')\n",
    "inception_4e_5_5 = conv_2d(inception_4e_5_5_reduce, 128,  filter_size=5, activation='relu', name='inception_4e_5_5')\n",
    "inception_4e_pool = max_pool_2d(inception_4d_output, kernel_size=3, strides=1,  name='inception_4e_pool')\n",
    "inception_4e_pool_1_1 = conv_2d(inception_4e_pool, 128, filter_size=1, activation='relu', name='inception_4e_pool_1_1')\n",
    "\n",
    "\n",
    "inception_4e_output = merge([inception_4e_1_1, inception_4e_3_3, inception_4e_5_5,inception_4e_pool_1_1],axis=3, mode='concat')\n",
    "\n",
    "pool4_3_3 = max_pool_2d(inception_4e_output, kernel_size=3, strides=2, name='pool_3_3')\n",
    "\n",
    "\n",
    "inception_5a_1_1 = conv_2d(pool4_3_3, 256, filter_size=1, activation='relu', name='inception_5a_1_1')\n",
    "inception_5a_3_3_reduce = conv_2d(pool4_3_3, 160, filter_size=1, activation='relu', name='inception_5a_3_3_reduce')\n",
    "inception_5a_3_3 = conv_2d(inception_5a_3_3_reduce, 320, filter_size=3, activation='relu', name='inception_5a_3_3')\n",
    "inception_5a_5_5_reduce = conv_2d(pool4_3_3, 32, filter_size=1, activation='relu', name='inception_5a_5_5_reduce')\n",
    "inception_5a_5_5 = conv_2d(inception_5a_5_5_reduce, 128, filter_size=5,  activation='relu', name='inception_5a_5_5')\n",
    "inception_5a_pool = max_pool_2d(pool4_3_3, kernel_size=3, strides=1,  name='inception_5a_pool')\n",
    "inception_5a_pool_1_1 = conv_2d(inception_5a_pool, 128, filter_size=1,activation='relu', name='inception_5a_pool_1_1')\n",
    "\n",
    "inception_5a_output = merge([inception_5a_1_1, inception_5a_3_3, inception_5a_5_5, inception_5a_pool_1_1], axis=3,mode='concat')\n",
    "\n",
    "\n",
    "inception_5b_1_1 = conv_2d(inception_5a_output, 384, filter_size=1,activation='relu', name='inception_5b_1_1')\n",
    "inception_5b_3_3_reduce = conv_2d(inception_5a_output, 192, filter_size=1, activation='relu', name='inception_5b_3_3_reduce')\n",
    "inception_5b_3_3 = conv_2d(inception_5b_3_3_reduce, 384,  filter_size=3,activation='relu', name='inception_5b_3_3')\n",
    "inception_5b_5_5_reduce = conv_2d(inception_5a_output, 48, filter_size=1, activation='relu', name='inception_5b_5_5_reduce')\n",
    "inception_5b_5_5 = conv_2d(inception_5b_5_5_reduce,128, filter_size=5,  activation='relu', name='inception_5b_5_5' )\n",
    "inception_5b_pool = max_pool_2d(inception_5a_output, kernel_size=3, strides=1,  name='inception_5b_pool')\n",
    "inception_5b_pool_1_1 = conv_2d(inception_5b_pool, 128, filter_size=1, activation='relu', name='inception_5b_pool_1_1')\n",
    "inception_5b_output = merge([inception_5b_1_1, inception_5b_3_3, inception_5b_5_5, inception_5b_pool_1_1], axis=3, mode='concat')\n",
    "\n",
    "pool5_7_7 = avg_pool_2d(inception_5b_output, kernel_size=7, strides=1)\n",
    "pool5_7_7 = dropout(pool5_7_7, 0.4)\n",
    "loss = fully_connected(pool5_7_7, 121,activation='softmax')\n",
    "\n",
    "network = regression(loss, optimizer='momentum',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     learning_rate=0.001)\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(network, checkpoint_path='model_inception',\n",
    "                max_checkpoints=1, tensorboard_verbose=0)\n",
    "model.fit(X_train, y_train, n_epoch=10, shuffle=False,\n",
    "      show_metric=True, batch_size=32, snapshot_step=500,\n",
    "      validation_set=(X_val, y_val), snapshot_epoch=False, run_id='vgg_plankton121_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model.save('cnn_1.tflearn')\n",
    "print (\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
