{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import tflearn\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected, flatten\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d, avg_pool_2d\n",
    "from tflearn.layers.estimator import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train.zip\n",
      "Extracted train.zip\n",
      "Extracting test.zip\n",
      "Extracted test.zip\n"
     ]
    }
   ],
   "source": [
    "# Parent folder where train and test images are saved\n",
    "main_dir = '/home/ubuntu/pynb/kaggle-ndsb1/data'\n",
    "\n",
    "def extract_data(filename, output_dir):\n",
    "    '''\n",
    "    Extracts zipped train and test files\n",
    "    '''\n",
    "    # Change working directory to the one containing train.zip & test.zip\n",
    "    os.chdir(output_dir)\n",
    "    \n",
    "    # Extracts zip files\n",
    "    print (\"Extracting \" + filename)\n",
    "    \n",
    "    fh = open(filename, 'rb')\n",
    "    z = zipfile.ZipFile(fh)\n",
    "    for name in z.namelist():\n",
    "        z.extract(name, output_dir)\n",
    "    fh.close()\n",
    "    print (\"Extracted \" + filename)\n",
    "        \n",
    "extract_data(\"train.zip\", main_dir)\n",
    "extract_data(\"test.zip\", main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing . . .\n",
      "Done data augmentation.\n"
     ]
    }
   ],
   "source": [
    "# Build numpy array of images, resize to 48x48 and shuffle dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "\n",
    "main_dir = '/home/ubuntu/pynb/kaggle-ndsb1/data'\n",
    "# Output folder where augmented images will be saved\n",
    "aug_dir = '/home/ubuntu/pynb/kaggle-ndsb1/data/aug'\n",
    "\n",
    "def img_aug(main_dir, aug_dir):\n",
    "    '''\n",
    "    Read images with OpenCV, perform augmentation and save the augmented images on disk\n",
    "    '''\n",
    "    # Create aug dir if does not exist\n",
    "    if not os.path.exists(aug_dir):\n",
    "        os.makedirs(aug_dir)\n",
    "    print (\"Initializing . . .\")\n",
    "    # Iterate over image classes\n",
    "    for pclass in os.listdir(main_dir + \"/train\"):\n",
    "        # Create aug/class dir if does not exist\n",
    "        if not os.path.exists(aug_dir + \"/\" + pclass):\n",
    "            os.makedirs(aug_dir + \"/\" + pclass)\n",
    "        # Iterate over images in each class\n",
    "        for pimg in os.listdir(main_dir + \"/train/\" + pclass):\n",
    "            img = cv2.imread(main_dir + '/train/' + pclass + '/' + pimg)\n",
    "            res = cv2.resize(img, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "            cv_gray = cv2.cvtColor(res, cv2.COLOR_RGB2GRAY)\n",
    "            rows,cols = cv_gray.shape\n",
    "            # Save base resized and grayscaled image\n",
    "            cv2.imwrite(aug_dir + \"/\" + pclass + \"/\" + pimg[:-4] + \"_base.jpg\", cv_gray)\n",
    "            # Perform 3 rotations and save them to disk separately\n",
    "            M1 = cv2.getRotationMatrix2D((cols/2,rows/2),90,1)\n",
    "            dst1 = cv2.warpAffine(cv_gray,M1,(cols,rows))\n",
    "            cv2.imwrite(aug_dir + \"/\" + pclass + \"/\" + pimg[:-4] + \"_aug1.jpg\", dst1)\n",
    "            M2 = cv2.getRotationMatrix2D((cols/2,rows/2),180,1)\n",
    "            dst2 = cv2.warpAffine(cv_gray,M2,(cols,rows))\n",
    "            cv2.imwrite(aug_dir + \"/\" + pclass + \"/\" + pimg[:-4] + \"_aug2.jpg\", dst2)\n",
    "            M3 = cv2.getRotationMatrix2D((cols/2,rows/2),270,1)\n",
    "            dst3 = cv2.warpAffine(cv_gray,M3,(cols,rows))\n",
    "            cv2.imwrite(aug_dir + \"/\" + pclass + \"/\" + pimg[:-4] + \"_aug3.jpg\", dst3)\n",
    "    print (\"Done data augmentation.\")\n",
    "\n",
    "img_aug(main_dir, aug_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing . . .\n",
      "Images sucessfully converted to dict.\n",
      "121344\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "aug_dir = '/home/ubuntu/pynb/kaggle-ndsb1/data/aug'\n",
    "\n",
    "def img_to_dict(aug_dir):\n",
    "    '''\n",
    "    Read images with OpenCV, perform transformation and save in dict\n",
    "    '''\n",
    "    print (\"Initializing . . .\")\n",
    "    # Initialize empty dict to contain each image as an array\n",
    "    img_dict = {}\n",
    "    # Iterate over image classes\n",
    "    for pclass in os.listdir(aug_dir):\n",
    "        # Iterate over images in a class\n",
    "        for pimg in os.listdir(aug_dir + \"/\" + pclass):\n",
    "            img = cv2.imread(aug_dir + '/' + pclass + '/' + pimg)\n",
    "            cv_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "            img_dict[pimg] = {pclass: cv_gray}\n",
    "    print (\"Images sucessfully converted to dict.\")\n",
    "    return img_dict\n",
    "\n",
    "img_dict = img_to_dict(aug_dir)\n",
    "print len(img_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acantharia_protist': array([[255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       ..., \n",
      "       [254, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 254, 254, ..., 255, 255, 255]], dtype=uint8)}\n"
     ]
    }
   ],
   "source": [
    "print img_dict['907_base.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "# Save dict\n",
    "import pickle\n",
    "with open('img_dict_2.pickle', 'wb') as handle:\n",
    "    pickle.dump(img_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print (\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load dict\n",
    "import pickle\n",
    "with open('img_dict_2.pickle', 'rb') as handle:\n",
    "    img_dict = pickle.load(handle)\n",
    "print (\"File loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((121344, 48, 48, 1), (121344, 121))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_dataset(img_dict):\n",
    "    '''\n",
    "    Constructs dataset of images as array and its labels.\n",
    "    Also normalizes pixel values by mean and std deviation.\n",
    "    '''\n",
    "    labels = [] # Need to change labels to numpy array\n",
    "    dataset = np.ndarray([len(img_dict),48,48,1], dtype='float32')\n",
    "    i = 0\n",
    "    # Iterate image and its class-array values pair\n",
    "    for img, pclass_arr in img_dict.iteritems():\n",
    "        \n",
    "        labels.append(pclass_arr.keys()[0])\n",
    "        im_array = pclass_arr.values()[0].reshape((48,48,1))\n",
    "        mean = np.mean(im_array, dtype='float32')\n",
    "        std = np.std(im_array, dtype='float32')\n",
    "        # Apply normalization to each image\n",
    "        im_array = (im_array - mean) / std \n",
    "        dataset[i,:,:,:] = im_array\n",
    "        i += 1\n",
    "        \n",
    "    # Encode labels\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    lb = LabelBinarizer()\n",
    "    labels = lb.fit_transform(labels)\n",
    "    \n",
    "    return dataset, labels\n",
    "    \n",
    "dataset, labels = prepare_dataset(img_dict)\n",
    "print (dataset.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-6.74140358]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  ..., \n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]]\n",
      "\n",
      " [[-6.74140358]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  ..., \n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]]\n",
      "\n",
      " [[-6.74140358]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  ..., \n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]]\n",
      "\n",
      " ..., \n",
      " [[-6.74140358]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  ..., \n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]]\n",
      "\n",
      " [[-6.74140358]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  ..., \n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]]\n",
      "\n",
      " [[-6.74140358]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  ..., \n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]\n",
      "  [ 0.18542044]]]\n"
     ]
    }
   ],
   "source": [
    "print dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "# Save dataset and labels\n",
    "import pickle\n",
    "with open('dataset.pickle', 'wb') as handle:\n",
    "    pickle.dump(dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(labels, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print (\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and labels\n",
    "import pickle\n",
    "with open('dataset.pickle', 'rb') as handle:\n",
    "    img_dict = pickle.load(handle)\n",
    "with open('labels.pickle', 'rb') as handle:\n",
    "    img_dict = pickle.load(handle)\n",
    "print (\"File loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([67139, 80980, 74225, ..., 12282, 35638, 11316]), 'VAL:', array([115058, 119818, 104462, ...,  85887,  69888,  18654]))\n",
      "('TRAIN:', array([ 65522,  39899, 116935, ..., 114485,  48610,  97844]), 'VAL:', array([81710, 73998, 38975, ..., 29230, 60841, 73892]))\n",
      "('TRAIN:', array([13844, 41144, 53608, ...,  4187, 87474, 81950]), 'VAL:', array([117790,  17974,  59888, ...,  18291,  39433,  94389]))\n"
     ]
    }
   ],
   "source": [
    "# Split training data to train and validation, then shuffle datasets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=3, test_size=0.1, random_state=42)\n",
    "\n",
    "for train_index, val_index in sss.split(dataset, labels):\n",
    "    print(\"TRAIN:\", train_index, \"VAL:\", val_index)\n",
    "    X_train, X_val = dataset[train_index], dataset[val_index]\n",
    "    y_train, y_val = labels[train_index], labels[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "# Save dataset and labels\n",
    "import pickle\n",
    "with open('train.pickle', 'wb') as handle:\n",
    "    pickle.dump([X_train, y_train], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('val.pickle', 'wb') as handle:\n",
    "    pickle.dump([X_val, y_val], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print (\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and labels\n",
    "import pickle\n",
    "with open('train.pickle', 'rb') as handle:\n",
    "    X_train, y_train = pickle.load(handle)\n",
    "with open('val.pickle', 'rb') as handle:\n",
    "    X_val, y_val = pickle.load(handle)\n",
    "print (\"File loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name image_preloader",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dc7337e95888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build the preloader array, resize images to 48x48, and shuffle dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage_preloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m X, Y = image_preloader('data/train/', image_shape=(48, 48), mode='folder',\n\u001b[1;32m      5\u001b[0m                        categorical_labels=True, grayscale=True, normalize=False)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name image_preloader"
     ]
    }
   ],
   "source": [
    "# Build the preloader array, resize images to 48x48, and shuffle dataset\n",
    "# Note: Error on importing preloader array since 1 Mar 2017. Thus, manual data aug was performed instead\n",
    "'''from tflearn.data_utils import image_preloader\n",
    "from tflearn.data_utils import shuffle\n",
    "X, Y = image_preloader('data/train/', image_shape=(48, 48), mode='folder',\n",
    "                       categorical_labels=True, grayscale=True, normalize=False)\n",
    "X, Y = shuffle(X, Y)\n",
    "\n",
    "print (X[0], Y[0])\n",
    "\n",
    "# Preprocess images\n",
    "img_prep = tflearn.ImagePreprocessing()\n",
    "# Zero Center (With mean computed over the whole dataset)\n",
    "img_prep.add_featurewise_zero_center()\n",
    "# STD Normalization (With std computed over the whole dataset)\n",
    "img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "# Create extra synthetic training data by flipping, rotating and blurring the\n",
    "# images on our data set.\n",
    "img_aug = tflearn.ImageAugmentation()\n",
    "img_aug.add_random_flip_leftright()\n",
    "img_aug.add_random_flip_updown()\n",
    "img_aug.add_random_rotation(max_angle=25.)\n",
    "img_aug.add_random_blur(sigma_max=2.)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tflearn/summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tflearn/summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.py:766 in create_summaries.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.py:130 in __init__.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.py:378 in restore.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for inception_1.tfl\n\t [[Node: save_1/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_1/Const_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]\n\t [[Node: save_1/RestoreV2_53/_155 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_982_save_1/RestoreV2_53\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op u'save_1/RestoreV2', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 160, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/lib/python2.7/dist-packages/tornado/ioloop.py\", line 866, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-2-78a6cd88ab61>\", line 153, in <module>\n    max_checkpoints=1, tensorboard_verbose=0)\n  File \"/usr/local/lib/python2.7/dist-packages/tflearn/models/dnn.py\", line 57, in __init__\n    session=session)\n  File \"/usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.py\", line 125, in __init__\n    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1030, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 624, in build\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 361, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 200, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 441, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for inception_1.tfl\n\t [[Node: save_1/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_1/Const_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]\n\t [[Node: save_1/RestoreV2_53/_155 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_982_save_1/RestoreV2_53\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-78a6cd88ab61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m model = tflearn.DNN(network, checkpoint_path='model_inception.ckpt',\n\u001b[1;32m    153\u001b[0m                 max_checkpoints=1, tensorboard_verbose=0)\n\u001b[0;32m--> 154\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inception_1.tfl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Model loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m model.fit(X_train, y_train, n_epoch=20, shuffle=False,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tflearn/models/dnn.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, model_file)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         self.predictor = Evaluator([self.net],\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, model_file)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1386\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1388\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for inception_1.tfl\n\t [[Node: save_1/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_1/Const_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]\n\t [[Node: save_1/RestoreV2_53/_155 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_982_save_1/RestoreV2_53\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op u'save_1/RestoreV2', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 160, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/lib/python2.7/dist-packages/tornado/ioloop.py\", line 866, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-2-78a6cd88ab61>\", line 153, in <module>\n    max_checkpoints=1, tensorboard_verbose=0)\n  File \"/usr/local/lib/python2.7/dist-packages/tflearn/models/dnn.py\", line 57, in __init__\n    session=session)\n  File \"/usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.py\", line 125, in __init__\n    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1030, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 624, in build\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 361, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 200, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 441, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for inception_1.tfl\n\t [[Node: save_1/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_1/Const_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]\n\t [[Node: save_1/RestoreV2_53/_155 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_982_save_1/RestoreV2_53\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected, flatten\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d, avg_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.merge_ops import merge\n",
    "\n",
    "# Model - Inception V3\n",
    "network = input_data(shape=[None, 48, 48, 1])\n",
    "conv1_7_7 = conv_2d(network, 64, 7, strides=2, activation='relu', name = 'conv1_7_7_s2')\n",
    "pool1_3_3 = max_pool_2d(conv1_7_7, 3,strides=2)\n",
    "pool1_3_3 = local_response_normalization(pool1_3_3)\n",
    "conv2_3_3_reduce = conv_2d(pool1_3_3, 64,1, activation='relu',name = 'conv2_3_3_reduce')\n",
    "conv2_3_3 = conv_2d(conv2_3_3_reduce, 192,3, activation='relu', name='conv2_3_3')\n",
    "conv2_3_3 = local_response_normalization(conv2_3_3)\n",
    "pool2_3_3 = max_pool_2d(conv2_3_3, kernel_size=3, strides=2, name='pool2_3_3_s2')\n",
    "inception_3a_1_1 = conv_2d(pool2_3_3, 64, 1, activation='relu', name='inception_3a_1_1')\n",
    "inception_3a_3_3_reduce = conv_2d(pool2_3_3, 96,1, activation='relu', name='inception_3a_3_3_reduce')\n",
    "inception_3a_3_3 = conv_2d(inception_3a_3_3_reduce, 128,filter_size=3,  activation='relu', name = 'inception_3a_3_3')\n",
    "inception_3a_5_5_reduce = conv_2d(pool2_3_3,16, filter_size=1,activation='relu', name ='inception_3a_5_5_reduce' )\n",
    "inception_3a_5_5 = conv_2d(inception_3a_5_5_reduce, 32, filter_size=5, activation='relu', name= 'inception_3a_5_5')\n",
    "inception_3a_pool = max_pool_2d(pool2_3_3, kernel_size=3, strides=1, )\n",
    "inception_3a_pool_1_1 = conv_2d(inception_3a_pool, 32, filter_size=1, activation='relu', name='inception_3a_pool_1_1')\n",
    "\n",
    "# merge the inception_3a__\n",
    "inception_3a_output = merge([inception_3a_1_1, inception_3a_3_3, inception_3a_5_5, inception_3a_pool_1_1], mode='concat', axis=3)\n",
    "\n",
    "inception_3b_1_1 = conv_2d(inception_3a_output, 128,filter_size=1,activation='relu', name= 'inception_3b_1_1' )\n",
    "inception_3b_3_3_reduce = conv_2d(inception_3a_output, 128, filter_size=1, activation='relu', name='inception_3b_3_3_reduce')\n",
    "inception_3b_3_3 = conv_2d(inception_3b_3_3_reduce, 192, filter_size=3,  activation='relu',name='inception_3b_3_3')\n",
    "inception_3b_5_5_reduce = conv_2d(inception_3a_output, 32, filter_size=1, activation='relu', name = 'inception_3b_5_5_reduce')\n",
    "inception_3b_5_5 = conv_2d(inception_3b_5_5_reduce, 96, filter_size=5,  name = 'inception_3b_5_5')\n",
    "inception_3b_pool = max_pool_2d(inception_3a_output, kernel_size=3, strides=1,  name='inception_3b_pool')\n",
    "inception_3b_pool_1_1 = conv_2d(inception_3b_pool, 64, filter_size=1,activation='relu', name='inception_3b_pool_1_1')\n",
    "\n",
    "#merge the inception_3b_*\n",
    "inception_3b_output = merge([inception_3b_1_1, inception_3b_3_3, inception_3b_5_5, inception_3b_pool_1_1], mode='concat',axis=3,name='inception_3b_output')\n",
    "\n",
    "pool3_3_3 = max_pool_2d(inception_3b_output, kernel_size=3, strides=2, name='pool3_3_3')\n",
    "inception_4a_1_1 = conv_2d(pool3_3_3, 192, filter_size=1, activation='relu', name='inception_4a_1_1')\n",
    "inception_4a_3_3_reduce = conv_2d(pool3_3_3, 96, filter_size=1, activation='relu', name='inception_4a_3_3_reduce')\n",
    "inception_4a_3_3 = conv_2d(inception_4a_3_3_reduce, 208, filter_size=3,  activation='relu', name='inception_4a_3_3')\n",
    "inception_4a_5_5_reduce = conv_2d(pool3_3_3, 16, filter_size=1, activation='relu', name='inception_4a_5_5_reduce')\n",
    "inception_4a_5_5 = conv_2d(inception_4a_5_5_reduce, 48, filter_size=5,  activation='relu', name='inception_4a_5_5')\n",
    "inception_4a_pool = max_pool_2d(pool3_3_3, kernel_size=3, strides=1,  name='inception_4a_pool')\n",
    "inception_4a_pool_1_1 = conv_2d(inception_4a_pool, 64, filter_size=1, activation='relu', name='inception_4a_pool_1_1')\n",
    "\n",
    "inception_4a_output = merge([inception_4a_1_1, inception_4a_3_3, inception_4a_5_5, inception_4a_pool_1_1], mode='concat', axis=3, name='inception_4a_output')\n",
    "\n",
    "\n",
    "inception_4b_1_1 = conv_2d(inception_4a_output, 160, filter_size=1, activation='relu', name='inception_4a_1_1')\n",
    "inception_4b_3_3_reduce = conv_2d(inception_4a_output, 112, filter_size=1, activation='relu', name='inception_4b_3_3_reduce')\n",
    "inception_4b_3_3 = conv_2d(inception_4b_3_3_reduce, 224, filter_size=3, activation='relu', name='inception_4b_3_3')\n",
    "inception_4b_5_5_reduce = conv_2d(inception_4a_output, 24, filter_size=1, activation='relu', name='inception_4b_5_5_reduce')\n",
    "inception_4b_5_5 = conv_2d(inception_4b_5_5_reduce, 64, filter_size=5,  activation='relu', name='inception_4b_5_5')\n",
    "\n",
    "inception_4b_pool = max_pool_2d(inception_4a_output, kernel_size=3, strides=1,  name='inception_4b_pool')\n",
    "inception_4b_pool_1_1 = conv_2d(inception_4b_pool, 64, filter_size=1, activation='relu', name='inception_4b_pool_1_1')\n",
    "\n",
    "inception_4b_output = merge([inception_4b_1_1, inception_4b_3_3, inception_4b_5_5, inception_4b_pool_1_1], mode='concat', axis=3, name='inception_4b_output')\n",
    "\n",
    "\n",
    "inception_4c_1_1 = conv_2d(inception_4b_output, 128, filter_size=1, activation='relu',name='inception_4c_1_1')\n",
    "inception_4c_3_3_reduce = conv_2d(inception_4b_output, 128, filter_size=1, activation='relu', name='inception_4c_3_3_reduce')\n",
    "inception_4c_3_3 = conv_2d(inception_4c_3_3_reduce, 256,  filter_size=3, activation='relu', name='inception_4c_3_3')\n",
    "inception_4c_5_5_reduce = conv_2d(inception_4b_output, 24, filter_size=1, activation='relu', name='inception_4c_5_5_reduce')\n",
    "inception_4c_5_5 = conv_2d(inception_4c_5_5_reduce, 64,  filter_size=5, activation='relu', name='inception_4c_5_5')\n",
    "\n",
    "inception_4c_pool = max_pool_2d(inception_4b_output, kernel_size=3, strides=1)\n",
    "inception_4c_pool_1_1 = conv_2d(inception_4c_pool, 64, filter_size=1, activation='relu', name='inception_4c_pool_1_1')\n",
    "\n",
    "inception_4c_output = merge([inception_4c_1_1, inception_4c_3_3, inception_4c_5_5, inception_4c_pool_1_1], mode='concat', axis=3,name='inception_4c_output')\n",
    "\n",
    "inception_4d_1_1 = conv_2d(inception_4c_output, 112, filter_size=1, activation='relu', name='inception_4d_1_1')\n",
    "inception_4d_3_3_reduce = conv_2d(inception_4c_output, 144, filter_size=1, activation='relu', name='inception_4d_3_3_reduce')\n",
    "inception_4d_3_3 = conv_2d(inception_4d_3_3_reduce, 288, filter_size=3, activation='relu', name='inception_4d_3_3')\n",
    "inception_4d_5_5_reduce = conv_2d(inception_4c_output, 32, filter_size=1, activation='relu', name='inception_4d_5_5_reduce')\n",
    "inception_4d_5_5 = conv_2d(inception_4d_5_5_reduce, 64, filter_size=5,  activation='relu', name='inception_4d_5_5')\n",
    "inception_4d_pool = max_pool_2d(inception_4c_output, kernel_size=3, strides=1,  name='inception_4d_pool')\n",
    "inception_4d_pool_1_1 = conv_2d(inception_4d_pool, 64, filter_size=1, activation='relu', name='inception_4d_pool_1_1')\n",
    "\n",
    "inception_4d_output = merge([inception_4d_1_1, inception_4d_3_3, inception_4d_5_5, inception_4d_pool_1_1], mode='concat', axis=3, name='inception_4d_output')\n",
    "\n",
    "inception_4e_1_1 = conv_2d(inception_4d_output, 256, filter_size=1, activation='relu', name='inception_4e_1_1')\n",
    "inception_4e_3_3_reduce = conv_2d(inception_4d_output, 160, filter_size=1, activation='relu', name='inception_4e_3_3_reduce')\n",
    "inception_4e_3_3 = conv_2d(inception_4e_3_3_reduce, 320, filter_size=3, activation='relu', name='inception_4e_3_3')\n",
    "inception_4e_5_5_reduce = conv_2d(inception_4d_output, 32, filter_size=1, activation='relu', name='inception_4e_5_5_reduce')\n",
    "inception_4e_5_5 = conv_2d(inception_4e_5_5_reduce, 128,  filter_size=5, activation='relu', name='inception_4e_5_5')\n",
    "inception_4e_pool = max_pool_2d(inception_4d_output, kernel_size=3, strides=1,  name='inception_4e_pool')\n",
    "inception_4e_pool_1_1 = conv_2d(inception_4e_pool, 128, filter_size=1, activation='relu', name='inception_4e_pool_1_1')\n",
    "\n",
    "\n",
    "inception_4e_output = merge([inception_4e_1_1, inception_4e_3_3, inception_4e_5_5,inception_4e_pool_1_1],axis=3, mode='concat')\n",
    "\n",
    "pool4_3_3 = max_pool_2d(inception_4e_output, kernel_size=3, strides=2, name='pool_3_3')\n",
    "\n",
    "\n",
    "inception_5a_1_1 = conv_2d(pool4_3_3, 256, filter_size=1, activation='relu', name='inception_5a_1_1')\n",
    "inception_5a_3_3_reduce = conv_2d(pool4_3_3, 160, filter_size=1, activation='relu', name='inception_5a_3_3_reduce')\n",
    "inception_5a_3_3 = conv_2d(inception_5a_3_3_reduce, 320, filter_size=3, activation='relu', name='inception_5a_3_3')\n",
    "inception_5a_5_5_reduce = conv_2d(pool4_3_3, 32, filter_size=1, activation='relu', name='inception_5a_5_5_reduce')\n",
    "inception_5a_5_5 = conv_2d(inception_5a_5_5_reduce, 128, filter_size=5,  activation='relu', name='inception_5a_5_5')\n",
    "inception_5a_pool = max_pool_2d(pool4_3_3, kernel_size=3, strides=1,  name='inception_5a_pool')\n",
    "inception_5a_pool_1_1 = conv_2d(inception_5a_pool, 128, filter_size=1,activation='relu', name='inception_5a_pool_1_1')\n",
    "\n",
    "inception_5a_output = merge([inception_5a_1_1, inception_5a_3_3, inception_5a_5_5, inception_5a_pool_1_1], axis=3,mode='concat')\n",
    "\n",
    "\n",
    "inception_5b_1_1 = conv_2d(inception_5a_output, 384, filter_size=1,activation='relu', name='inception_5b_1_1')\n",
    "inception_5b_3_3_reduce = conv_2d(inception_5a_output, 192, filter_size=1, activation='relu', name='inception_5b_3_3_reduce')\n",
    "inception_5b_3_3 = conv_2d(inception_5b_3_3_reduce, 384,  filter_size=3,activation='relu', name='inception_5b_3_3')\n",
    "inception_5b_5_5_reduce = conv_2d(inception_5a_output, 48, filter_size=1, activation='relu', name='inception_5b_5_5_reduce')\n",
    "inception_5b_5_5 = conv_2d(inception_5b_5_5_reduce,128, filter_size=5,  activation='relu', name='inception_5b_5_5' )\n",
    "inception_5b_pool = max_pool_2d(inception_5a_output, kernel_size=3, strides=1,  name='inception_5b_pool')\n",
    "inception_5b_pool_1_1 = conv_2d(inception_5b_pool, 128, filter_size=1, activation='relu', name='inception_5b_pool_1_1')\n",
    "inception_5b_output = merge([inception_5b_1_1, inception_5b_3_3, inception_5b_5_5, inception_5b_pool_1_1], axis=3, mode='concat')\n",
    "\n",
    "pool5_7_7 = avg_pool_2d(inception_5b_output, kernel_size=7, strides=1)\n",
    "pool5_7_7 = dropout(pool5_7_7, 0.4)\n",
    "loss = fully_connected(pool5_7_7, 121,activation='softmax')\n",
    "\n",
    "network = regression(loss, optimizer='momentum',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     learning_rate=0.001)\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(network, checkpoint_path='model_inception.ckpt',\n",
    "                max_checkpoints=1, tensorboard_verbose=0)\n",
    "\n",
    "model.fit(X_train, y_train, n_epoch=40, shuffle=False,\n",
    "      show_metric=True, batch_size=32, snapshot_step=500,\n",
    "      validation_set=(X_val, y_val), snapshot_epoch=False, run_id='vgg_plankton121_7')\n",
    "\n",
    "# Save model\n",
    "model.save('inception_1.tfl')\n",
    "print (\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
